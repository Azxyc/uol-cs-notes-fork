I"…<p>Intelligent agents are classified depending on how they map their precepts to their actions. They can be classed under:</p>

<ul>
  <li>Simple reflex agents
    <ul>
      <li>Select actions to execute based upon the current percept</li>
      <li>Donâ€™t take the percept history into account.</li>
      <li>Very simple IFTTT actions.</li>
      <li>Very simple to implement but have limited intelligence.</li>
      <li>They have no memory.</li>
    </ul>
  </li>
</ul>

<p>@figure</p>
<pre><code class="language-mermaid">graph TB
    A(Environment)--&gt; |Sensors| B
    B[What the world is like now] --&gt; C
    C[What action I should do now]--&gt; |Actuators| A
    D[Condition-action rules]--&gt;C
</code></pre>
<p>@/</p>

<ul>
  <li>Model-based reflex agents
    <ul>
      <li>Maintain an internal state that depends upon the percept history - memory.</li>
      <li>Helps deal with partial observability.</li>
      <li>Knows how the world evolves and how its actions affect the world.</li>
    </ul>
  </li>
</ul>

<p>@figure</p>
<pre><code class="language-mermaid">graph TB
    A(Environment)--&gt; |Sensors| B
    B[What the world is like now] --&gt; C
    C[What action I should do now]--&gt; |Actuators| A
    D[Condition-action rules]--&gt;C
    E--&gt;B
    B.-&gt;E(State)
    F(How the world evolves)--&gt;B
    G(What my actions do)--&gt;B
</code></pre>
<p>@/</p>

<ul>
  <li>Goal-based agents
    <ul>
      <li>Select appropriate actions to achieve desirable state of the environment.</li>
      <li>Knowledge of the current state does not automatically mean that the agent knows what to do.</li>
      <li>It will have to plan what to do over a long sequence of actions.</li>
    </ul>
  </li>
</ul>

<p>@figure</p>
<pre><code class="language-mermaid">graph TB
    A(Environment)  --&gt; |Sensors| B
    B[What the world is like now] --&gt; H
    H[What it will be like if I do action A] --&gt; C
    C[What action I should do now]--&gt; |Actuators| A
    D[Condition-action rules]--&gt;C
    B.-&gt;E(State)
    E--&gt;B
    F(How the world evolves)--&gt;B
    F --&gt; H
    G(What my actions do)--&gt;B
    G --&gt; H
</code></pre>
<p>@/</p>

<ul>
  <li>Utility-based agents
    <ul>
      <li>Make use of a utility function to compare the <strong>desirability</strong> of different states that result from actions.</li>
      <li>Many actions may satisfy the goal but which one is the most desirable?</li>
      <li>Utility function maps a state, or sequence of states, onto a number to give the degree of <strong>usefulness</strong> of the state to the agents.</li>
      <li>Agent maximises the value of its utility function.</li>
    </ul>
  </li>
</ul>

<p>@figure</p>
<pre><code class="language-mermaid">graph TB
    A(Environment)--&gt; |Sensors| B
    B[What the world is like now] --&gt; H
    H[What it will be like if I do action A] --&gt; I
    I[How happy I will be in such a state] --&gt; C
    J(Utility)--&gt;I
    C[What action I should do now]--&gt; |Actuators| A
    D[Condition-action rules]--&gt;C
    B.-&gt;E(State)
    E--&gt;B
    F(How the world evolves)--&gt;B
    F --&gt; H
    G(What my actions do)--&gt;B
    G --&gt; H
</code></pre>
<p>@/</p>

<ul>
  <li>Learning agents
    <ul>
      <li>Learning agents find ways of maximising their utility function based on previous experience</li>
      <li>The performance element is the same choice mechanism from the previous agents. This is now informed by the learning element.</li>
      <li>The performance standard informs the learning element how well itâ€™s actions have worked.</li>
    </ul>
  </li>
</ul>

<p>@figure</p>
<pre><code class="language-mermaid">graph TB
	A(Environment) --&gt; |Sensors| B[Performance element]
	B --&gt; |Actuators| A
	D[Performance standard] --&gt; C[Critic]
	C --&gt; |Feedback| L[Learning element]
	L --&gt; |Changes| B
	B --&gt; |Knowledge| L
	L --&gt; |Learning Goals| P[Problem Generator]
	P --&gt; B
</code></pre>
<p>@/</p>
:ET