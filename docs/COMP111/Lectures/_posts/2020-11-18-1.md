---
title: COMP111 - Reasoning Under Uncertainty
tags: COMP111 Lectures
---
Logic based knowledge representation and reasoning methods mostly assume that knowledge is certain. Often, this is not the case (or it is impossible to list all assumptions that make it certain):

* When going to the airport by car, how early should I start? 45 minutes should be enough from Liverpool to Manchester Airport, but only under the assumption that there are no accidents, no lane closures, that my car does not break down, and so on.
* A dental patient has a toothache. Does the patient have a cavity? You might say:

	{% raw %}\]{% endraw %}\text{Toothache}(x)\rightarrow\text{Cavity}(x)\]

	This is not right as there are many factors that play into this and not just the fact that they have a toothache.
	
## Uncertainty
Trying to use exact rules to cope with a domain like medical diagnosis or traffic fails for three main reasons:

* Laziness
	* It is too much work to list an exception-less set of rules.
* Theoretical ignorance
	* Medical science has, in many cases, no strict laws connecting symptoms with diseases.
* Practical ignorance
	* Even if we have strict laws, we might be uncertain about a particular patient because not all the necessary tests have been or can be run.
	
## Probability in AI

Probability provides a way of summarising the uncertainty that comes form our laziness and ignorance.

We might not know for sure what disease a particular patient has, but we believe that there is an 80% chance that a patient with toothache has a cavity. The 80% summarises those cases in which all the factors needed for a cavity to cause a toothache are present and other cases in which the patient has both toothache and cavity but the two are unconnected.

The missing 20% summarises all the other possible causes we are too lazy or ignorant to find.

## Discrete Probability
We represent random experiments using discrete probability spaces {% raw %}\((S,P)\){% endraw %} consisting of:

* The sample space {% raw %}\(S\){% endraw %} of all elementary events {% raw %}\(x\in S\){% endraw %}. Members of {% raw %}\(S\){% endraw %} are also called outcomes of the experiment.
* A probability distribution {% raw %}\(P\){% endraw %} assigning a real number {% raw %}\(P(x)\){% endraw %} to every elementary event {% raw %}\(x\in S\){% endraw %} such that:
	* For every {% raw %}\(x\in S: 0\leq P(x) \leq 1\){% endraw %}
	* And {% raw %}\(\sum_{x\in S}P(x)=1\){% endraw %}
	
Recall that if {% raw %}\(S\){% endraw %} consists of {% raw %}\(x_1,\ldots,x_n\){% endraw %}, then:

{% raw %}\]{% endraw %}\sum_{x\in S}P(x)=P(x_1)+\ldots+P(x_n)\]

### Example - Flipping a Fair Coin
Consider the random experiment of flipping a coin. The then corresponding probability space {% raw %}\((S,P)\){% endraw %} is given by:

* {% raw %}\(S=\{H,T\}\){% endraw %}
* {% raw %}\(P(H)=P(T)=\frac{1}{2}\){% endraw %}

Consider the random experiment of flipping a count two times, one after the other. Then the corresponding probability space {% raw %}\((S,P)\){% endraw %} is:

* {% raw %}\(S=\{HH,HT,TH,TT\}\){% endraw %}
* {% raw %}\(P(HH)=P(HT)=P(TH)=P(TT)=\frac{1}{4}\){% endraw %}

### Example - Rolling a fair die
Consider the random experiment of rolling a die. Then the corresponding probability space {% raw %}\((S, P)\){% endraw %} is given by:

* S = {1, 2, 3, 4, 5, 6};
* For every {% raw %}\(x âˆˆ S: P(x) = \frac{1}{6}\){% endraw %}

Consider the random experiment of rolling a die {% raw %}\(n\){% endraw %} times. Then the corresponding probability space {% raw %}\((S, P)\){% endraw %} is given as follows:

* {% raw %}\(S\){% endraw %} is the set of sequences of length {% raw %}\(n\){% endraw %} over the alphabet {% raw %}\(\{1,\ldots, 6\}\){% endraw %}
	* Sometimes denoted {% raw %}\(\{1,\ldots, 6\}^n\){% endraw %}
* {% raw %}\(P(x) = \frac{1}{6^n}\){% endraw %} for every elementary event {% raw %}\(x\){% endraw %}, since {% raw %}\(S\){% endraw %} has {% raw %}\(6^n\){% endraw %} elements.

## Uniform Probability Distributions
A probability distribution is uniform if every outcome is equally likely. For uniform probability distributions, the probability of an outcome {% raw %}\(x\){% endraw %} is 1 divided by the number {% raw %}\(\vert S\vert??????of outcomes in \){% endraw %}S{% raw %}\(:

{% raw %}\]{% endraw %}P(x)=\frac{1}{\vert S\vert }\]

