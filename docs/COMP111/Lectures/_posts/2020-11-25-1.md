---
title: COMP111 - Joint Probability Distribution
tags: COMP111 Lectures
---
## Examples of Probabilistic Models
To model a domain using probability theory, one first introduces the relevant random variable. We have seen two basic examples:

* The weather domain could be modelled using the single random variable {% raw %}<pre>\(\text{Weather}\)</pre>{% endraw %} with values:

	{% raw %}<pre>\[(\text{sunny},\text{rain},\text{cloudy},\text{snow})\]</pre>{% endraw %}

* The dentist domain could be modelled using the random variables {% raw %}<pre>\(\text{Toothache, Cavity}\)</pre>{% endraw %} and {% raw %}<pre>\(\text{Catch}\)</pre>{% endraw %} with values 0 and 1 for True or False. We might be interested in:

{% raw %}<pre>\[P(\text{Cavity}=1\vert\text{Toothache}=1,\text{Catch}=1)\]</pre>{% endraw %}

## Probability Distribution of a Single Random Variable
The probability distribution for a random variable gives the probabilities of all the possible values of the random variable.

For example let {% raw %}<pre>\(\text{Weather}\)</pre>{% endraw %} be a random variable with values:

{% raw %}<pre>\[(\text{sunny},\text{rain},\text{cloudy},\text{snow})\]</pre>{% endraw %}

such that its probability distribution is given by:

* {% raw %}<pre>\(P(\text{Weather}=\text{sunny})=0.7\)</pre>{% endraw %}
* {% raw %}<pre>\(P(\text{Weather}=\text{rain})=0.2\)</pre>{% endraw %}
* {% raw %}<pre>\(P(\text{Weather}=\text{cloudy})=0.08\)</pre>{% endraw %}
* {% raw %}<pre>\(P(\text{Weather}=\text{snow})=0.02\)</pre>{% endraw %}

Assume the order of the values is fixed. The we write instead:

{% raw %}<pre>\[\mathbf{P}(\text{Weather})=(0.7,0.2,0.008,0.02)\]</pre>{% endraw %}

Where the bold {% raw %}<pre>\(\mathbf{P}\)</pre>{% endraw %} indicates that the result is a vector of number representing the individual values of {% raw %}<pre>\(\text{Weather}\)</pre>{% endraw %}.

We can write the values as a vector in the case that the properties are ordered.
{:.info}

## Joint Probability Distribution
This is the case when you are using many random variables.

Let {% raw %}<pre>\(F_1,\ldots,F_k\)</pre>{% endraw %} be random variable. A joint probability distribution for:

{% raw %}<pre>\[F_1,\ldots,F_k\]</pre>{% endraw %}

gives the probabilities:

{% raw %}<pre>\[P(F_1=r_1,\ldots,F_k=r_k)\]</pre>{% endraw %}

for the event:

{% raw %}<pre>\[(F_1=r_1)\text{ and } \cdots \text{ and }(F_k=r_k)\]</pre>{% endraw %}

that {% raw %}<pre>\(F_1\)</pre>{% endraw %} takes value {% raw %}<pre>\(r_1\)</pre>{% endraw %}, {% raw %}<pre>\(F_2\)</pre>{% endraw %} take value {% raw %}<pre>\(r_2\)</pre>{% endraw %}, and so on up to {% raw %}<pre>\(k\)</pre>{% endraw %} , for all possible values {% raw %}<pre>\(r_1,\ldots,r_k\)</pre>{% endraw %}.

The joint probability distribution is denotes {% raw %}<pre>\(\mathbf{P}(F_1,\ldots,K_k)\)</pre>{% endraw %}.

### Example
A possible joint probabillity distribution {% raw %}<pre>\(\mathbf{P}(\text{Weather, Cavity})\)</pre>{% endraw %} for the random varables {% raw %}<pre>\(\text{Weather}\)</pre>{% endraw %} and {% raw %}<pre>\(\text{Cavity}\)</pre>{% endraw %} is given by the following table:

| {% raw %}<pre>\(\text{Weather}=\)</pre>{% endraw %} | {% raw %}<pre>\(\text{sunny}\)</pre>{% endraw %} | {% raw %}<pre>\(\text{rain}\)</pre>{% endraw %} | {% raw %}<pre>\(\text{cloudy}\)</pre>{% endraw %} | {% raw %}<pre>\(\text{snow}\)</pre>{% endraw %} |
| :-: |  :-: | :-: | :-: | :-: |
| {% raw %}<pre>\(\text{Cavity}=1\)</pre>{% endraw %} | 0.144 | 0.02 | 0.016 | 0.02 |
| {% raw %}<pre>\(\text{Cavity}=0\)</pre>{% endraw %} | 0.576 | 0.08 | 0.064 | 0.08 |

For this to be plausible we should know that the two events are independent of each-other.
{:.warning}

### Full Joint Probability Distribution

A full joint probability distribution:

{% raw %}<pre>\[\mathbf{P}(F_1,\ldots,K_k)\]</pre>{% endraw %}

is a joint probability distribution for all relavent random variables {% raw %}<pre>\(F_1,\ldots,F_k\)</pre>{% endraw %} for a domain of interest.

Every probability quesetion about a domain can be answered by tehf ull joint distributin because the probability of every evenint is a sum of the probabilities:

{% raw %}<pre>\[P(F_1=r_1,\ldots,F_k=r_k)\]</pre>{% endraw %}

The {% raw %}<pre>\(r_1,\ldots,r_k\)</pre>{% endraw %} are often called data points or sample points.
{:.info}

#### Example
Assume the random variables {% raw %}<pre>\(\text{Toothache, Cavity, Catch}\)</pre>{% endraw %} fully describe a visit to a dentist. 

The a full joint probability distribution is gien by the following table:

| | {% raw %}<pre>\(\text{Toothache}=1\)</pre>{% endraw %} | {% raw %}<pre>\(\text{Toothache}=1\)</pre>{% endraw %} | {% raw %}<pre>\(\text{Toothache}=0\)</pre>{% endraw %} | {% raw %}<pre>\(\text{Toothache}=0\)</pre>{% endraw %} |
| :-: | :-: | :-: | :-: | :-: |
| | {% raw %}<pre>\(\text{Catch}=1\)</pre>{% endraw %} |  {% raw %}<pre>\(\text{Catch}=0\)</pre>{% endraw %} |  {% raw %}<pre>\(\text{Catch}=1\)</pre>{% endraw %} |  {% raw %}<pre>\(\text{Catch}=0\)</pre>{% endraw %} | 
| {% raw %}<pre>\(\text{Cavity}=1\)</pre>{% endraw %} | 0.108 | 0.012 | 0.072 | 0.008 |
| {% raw %}<pre>\(\text{Cavity}=0\)</pre>{% endraw %} | 0.016 | 0.064 | 0.144 | 0.576 |
