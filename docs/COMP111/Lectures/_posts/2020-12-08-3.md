---
title: COMP111 - Naive Bayes Classifier - 2
tags: COMP111 Lectures
---
Suppose we want to predict the class of the instance with features &#92;((e_1,\ldots,e_n)&#92;).

We assume &#92;(E_1,\ldots,E_n&#92;) are independent given &#92;(Y&#92;) and estimate:

&#92;[V_y=P(Y=y)\prod^n_{i=1}P(E_i=e_i\vert Y=y)&#92;]

The &#92;(\prod&#92;) symbol means the product of.
{:.info}

for all &#92;(y\in \mathcal Y&#92;) as follows:

* &#92;(P(Y=y)&#92;) is estimated by the number of instance labelled with &#92;(y&#92;) in the training data divided by the number of all instances in the training data.
* &#92;(P(E_i=e_i\vert Y=y)&#92;) is estimated by the number of instances with feature &#92;(e_i&#92;) labelled as &#92;(y&#92;) in the training data divided by the number of all instances labelled with &#92;(y&#92;) in the training data.

We then set:

&#92;[f(e_1,\ldots,e_n)=y&#92;]

for the &#92;(y&#92;) for which &#92;(V_y&#92;) is maximal (take the maximally probability hypothesis).

View [side 27 onwards]({{site.baseurl}}/assets/COMP111/Lectures/2020-12-08-3.pdf) for an additional example about filtering spam emails.
{:.info}

## Summary 

* A supervised learning algorithm based on Bayes Theorem.
* It is called naive because it is assumed that the features are independent of each other, given the classification.
* Naive Bayes classifier works surprising well in practice even if the features are obviously not independent given the classification.
