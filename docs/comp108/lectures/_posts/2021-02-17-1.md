---
title: COMP108 - Algorithm Efficiency - 1
tags: COMP108 Lectures
---
Efficiency matters as we want to make more ambitious code to leverage increases in computational power. If we didn't optimise code then we'd be wasting resources.

## Time Complexity Analysis
Timing execution time is not enough as it is not reproducible and wastes time.

We should identify some important operations and count how many times these operation need to be executed.

We can then change the input size and see how the time the algorithm takes changes.

### Data vs Speed Increase
Assume this initial scenario:


* An algorithm takes &#92;(n^2&#92;) comparisons to sort &#92;(n&#92;) numbers.
* We need 1 second to sort 5 numbers (25 comparisons).

Now suppose that Computing speed increases by a factor of 100:

* This means in 1 second we can now do 2500 comparisons.
* This gives a result of 50 numbers vs the original 5.

We show here that an &#92;(n^2&#92;) comparison algorithm gives a 10 times speed increase with a raw 100 times speed increase.
{:.info}

### Important Operations
Here is an algorithm:

```
sum = 0
i = 1
while i <= n do
begin
	sum = sum  + i
	i++
end 
output sum
```

The important operation of summation is addition.

For this algorithm we need &#92;(2n&#92;) additions (dependant on the input size of &#92;(n&#92;)).
{:.info}

### Which Algorithm is Fastest?
This question may be dependant on the input size you may want to use several algorithms:

![](/home/ben/Documents/University/COMP108/Lectures/Week 2/2021-02-17-1.md.1409.png)

* &#92;(f_1(n) = n^2 -3n+6&#92;)
* &#92;(f_2(n) = 50n+20&#92;)

Generally you would want to choose the most efficient algorithm for when the input is maximal. In this example you would choose &#92;(f_2(n)&#92;).

### Growth Rate

Here are functions ordered by their growth rate:

* Constant
	* 1
* Logarithm
	* &#92;(\log n&#92;)
* Polynomial
	* &#92;(\sqrt n&#92;)
	* &#92;(n&#92;)
* Polynomial Logarithm (In-between each polynomial.)
	* &#92;(n\log n&#92;)
* Polynomial
	* &#92;(n^2&#92;)
	* &#92;(n^3&#92;)
* Exponential
	* &#92;(2^n&#92;)

In computer science &#92;(\log&#92;) refers to &#92;(\log_2&#92;).
{:.warning}

#### Other Hierarchies
Comparing &#92;(\log^3n&#92;) and &#92;(n&#92;).

Identity:
&#92;[n\equiv 2^{\log_2n}&#92;]

As &#92;(\log^3n&#92;) is polynomial in terms of &#92;(\log n&#92;) and &#92;(n&#92;) (by the identity) is exponential in terms of &#92;(\log n&#92;) the former is lower in the hierarchy.

Similarly, &#92;(\log^kn&#92;) is lower than &#92;(n&#92;) in the hierarchy, for any constant &#92;(k&#92;).
{:.info}

## Big-O Notation
&#92;[f(n) =O(g(n))&#92;]

Read as &#92;(f&#92;) of &#92;(n&#92;) is of order &#92;(g&#92;) of &#92;(n&#92;).
{:.info}

Roughly speaking, this means that &#92;(f(n)&#92;) is at most a constant times &#92;(g(n)&#92;) for all large &#92;(n&#92;):

* &#92;(2n^3=O(n^3)&#92;)
* &#92;(3n^2=O(n^2)&#92;)
* &#92;(2n\log n=O(n\log n)&#92;)
* &#92;(n^3+n^2=O(n^3)

When we have an algorithm, we can then measure its time complexity by:

* Counting number of operations in therms of the input size.
* Expressing it using big-O notation.
