---
title: COMP111 - Nearest Neighbour Classifier - 2
tags: COMP111 Lectures
---
## &#92;(k&#92;)-Nearest Neighbour Classifier
For &#92;(x&#92;) to be classifier find &#92;(k&#92;) nearest &#92;(x_{i_1},\ldots,x_{i_k}&#92;) in the training data.

Classify &#92;(x&#92;) according to the majority vote of their class labels.

For &#92;(k=3&#92;):

![]({{site.baseurl}}/assets/comp111/lectures/2020-12-10-2-1.png)

The &#92;(k&#92;) represents the number of closest labels you are taking into account.
{:.info}

```
1: Input: training data (x0, L(x0)), ... , (xn, L(xn))
2:     new x âˆˆ  X to be classified
3:
4: for i = 0 to n do
5:     Compute distance d(x, xi)
6: endfor
7: Let xi1, ... , xik be the list of items such that
8:     d(x, xij) is among the k smallest distances
9: return label L which occurs most frequently in L(xi1), ... , L(xik) (majority vote)
```

### Learning a "good" &#92;(k&#92;)
To learn a "good" &#92;(k&#92;) divide labelled data into **training data** &#92;(T&#92;) and **validation data** &#92;(V&#92;).

This will result in the following two subsets of the labelled data:

![]({{site.baseurl}}/assets/comp111/lectures/2020-12-10-2-2.png)

The left is the training data and the right is the validation data.
{:.info}

The **classification error** of a classifier &#92;(f&#92;) on a set of data items is the number of incorrectly classified data items divided by the total number of data items.

We are saying here that we want a &#92;(k&#92;) such that new items added have the least classification error.
{:.info}

Let:

* &#92;(f_1&#92;) be the 1-nearest neighbour classifier obtained from the training data &#92;(T&#92;).
* &#92;(f_2&#92;) be the 2-nearest neighbour classifier obtained from the training data &#92;(T&#92;).
* and so on for &#92;(3,\ldots,m&#92;).

Choose &#92;(f_k&#92;) for which the classification error is minimal on the **validation data** (not the training data).

#### Generalisation

* The aim of supervised learning is to do well on test data that is not know during learning.
* Choosing the values for parameters (here &#92;(k&#92;)) that minimise the classification error on the training data is not necessarily the best policy.
* We want the learning algorithm to model true regularities in the data and ignore noise in the data.

#### Training and Validation Data - &#92;(k=1&#92;)

![]({{site.baseurl}}/assets/comp111/lectures/2020-12-10-2-3.png)

If we chose the value that was best for the training data then we can see here that we would almost always go for &#92;(k=1&#92;). However, we want to be good generally so we optimise &#92;(k&#92;) for the test data.

#### Training and Validation Data - &#92;(k=7&#92;)

![]({{site.baseurl}}/assets/comp111/lectures/2020-12-10-2-4.png)

You can see that although the error of the training data goes up the "general" error (on the validation data) goes down. We want to increase &#92;(k&#92;) to  minimise this value.

The line also gets smoother as &#92;(k&#92;) increases as it is interpolating the data and removing noise.
{:.info}

#### Properties and Training
As &#92;(k&#92;) increases:

* Classification boundaries become smother (possibly reflecting regularity in the data.
* Error on the training data can increase.

## Summary
### Advantages

* &#92;(k&#92;)-nearest neighbour is simple but effective.
* Decision surface can be non-linear.
* Only a single parameter, &#92;(k&#92;), easily learned by cross validation.

### Disadvantages

* What does nearest mean?
	* Need to specify a distance measure.
* Computational cost.
	* Must store and sear through the entire training set at test time.
