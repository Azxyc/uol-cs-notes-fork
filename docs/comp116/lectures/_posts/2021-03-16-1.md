---
title: COMP116 - Statistical Measures - Expectation, Variance & Standard Deviation
tags: COMP116 Lectures
---
## Random Variables & Measures
A random variable is just a real valued function over a population:

&#92;[r:\Omega\rightarrow\Bbb R&#92;]

Using a probability distribution:

&#92;[P:\Omega\rightarrow [0,1]&#92;]

we can find the **expected value** &#92;(E[X]&#92;) of a random variable. 

This is also sometimes called the **average** value or the **mean** value.
{:.info}

This is calculated using the following summation:

&#92;[E[X]=\sum&#95;&#123;X\in\Omega}P[X]r(X)&#92;]

This is the probability of choosing an element multiplied by the value of the associated random variable.
{:.info}

This &#92;(E[X]&#92;) represents the **typical** value or what we **expect** to happen.

### Dice Example
The expected value of throwing a fair die is &#92;(\frac{7}{2}&#92;). This is calculated by:

&#92;[E[X]=\sum^6&#95;&#123;k=1}\frac{1}{6}k=\frac{21}{6}=3.5&#92;]

Suppose we are using a biased die with the following probabilities:

&#92;[
P[x]=\begin{cases}\frac14 &\text{if }X\in&#92;{1,2,3&#92;}&#92;&#92;\frac1{12}&\text{if }X\in&#92;{4,5,6&#92;}\end{cases}
&#92;]

We can then work out the expected value to be:

&#92;[E[X]=\frac64+\frac{15}{12}=\frac{11}4=2.75&#92;]

## Mode & Median
Sometimes means can be misleading due to outliers; mode and median don't take into account these outliers. You calculate them as you expect.

## Variance
You may find that for a dice (with the same population and basis for random variables) you get distinct outcomes.

The idea of variance allows you to see the spread of the random variables are.

### Formal Definition
We have a population &#92;(\Omega&#92;) and a random variable &#92;(r(X)&#92;); leading to expectation &#92;(E[X]&#92;) using probability distribution &#92;(P[X]&#92;).

The variance &#92;(\text{Var}(X)&#92;) is defined to be:

&#92;[\sum&#95;&#123;X\in\Omega}(r(X)-E[X])^2&#92;]

Variance gives a measure of by how much the population as a whole differs from the expected value.
{:.info}

This is always non-negative.
{:.info}

## Standard Deviation
Formally the **exact standard deviation** is:

&#92;[\sigma \equiv\sqrt{\text{Var}(X)}&#92;]

The variance is defined with respect to the whole population but it is not always feasible to compute that.
{:.info}

### Estimated Standard Deviation
One way of estimating is by using a sampling method according the the **probability distribution**.

We take &#92;(N&#92;) samples from &#92;(\Omega&#92;):

&#92;[\langle y_1,y_2,\ldots,y_N\rangle:y_k=r(X_k)&#92;]

The **estimated standard deviation** is:

&#92;[S_N\equiv\sqrt{\frac{\sum^N&#95;&#123;i=1}(y_i-E[Y])^2}N}&#92;]

This is the variance of the sample, divided by the number of samples chosen. This is then rooted to find the standard deviation.
{:.info}

Notice that:

&#92;[E[Y]=\frac{\sum^N&#95;&#123;i=1}y_i}N&#92;]

This method captures how the population behaves b looking at a sample of its members.

This method sometimes gives underestimates.
{:.warning}

#### Bessel's Correction
This method attempts to correct for underestimates when estimating standard deviation.

Take &#92;(N&#92;) sample from &#92;(\Omega&#92;):

&#92;[\langle y_1,y_2,\ldots,y_N\rangle:y_k=r(X_k)&#92;]

The Bessel's correction for estimated standard deviation:

&#92;[S^B_N\equiv\sqrt{\frac{\sum^N&#95;&#123;i=1}(y_i-E[Y])^2}{N-1}}&#92;]
 
The change is that we divide by the number of samples minus one.
{:.info}

## Significance Testing
A typical experiment will have:

* A **predicted outcome**: &#92;(X&#92;)
* An **actual outcome**: &#92;(Y&#92;)

We want to know the chance of out prediction begin accurate given the outcome is likely.

To assess this we count the number of standard deviations by which &#92;(Y&#92;) differs from &#92;(X&#92;). If there are too many then the **hypothesis** isn't valid.
