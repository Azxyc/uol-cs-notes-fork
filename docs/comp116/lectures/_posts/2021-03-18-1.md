---
title: COMP116 - Regression & Data Analysis
tags: COMP116 Lectures
---
This is the method that we use to draw trends from experimental data. This is better than a table as it makes the data clearer.

## Example
How to we best fit a line to this graph:

```chart
{
	"type": "scatter",
	"data": {
		"datasets": [{
			"label": "Scatter Dataset",
			"data": [
				{x: 1.5,
				y: 1.2},
				{x: 2.4,
				y: 1.5},
				{x: 3.6,
				y: 1.9},
				{x: 4.2,
				y: 2.0},
				{x: 4.8,
				y: 2.2},
				{x: 5.7,
				y: 2.4},
				{x: 6.3,
				y: 2.5},
				{x: 7.9,
				y: 2.8
		    }]
		}]
	},
	"options": {
		"scales": {
		    "xAxes": [{
		        "type": "linear",
		        "position": "bottom"
		    }]
		}
	}
}
```

It seems that a curve would be best suited.
{:.info}

## Least Squares
The standard interpretation of best fit is **least squares**.

We have data observation pairs:

&#92;[&#92;{\langle x_k,y_k\rangle:1\leq k\leq n&#92;}&#92;]

We want a function &#92;(F:\Bbb R\rightarrow \Bbb R&#92;) that best describes these (minimises):

&#92;[\sum^n&#95;&#123;k=1}(y_k-F(x_k))^2&#92;]

This gives smaller values if the function fits the data better.
{:.info}

## Linear Functions & Regression
This function is in the following form:

&#92;[F(x)=ax+b&#92;]

This means that finding the best line is finding the combination of &#92;(a&#92;) and &#92;(b&#92;) which results in the least squares sum.

### Applying Least Squares
To minimise we would use the following function:

&#92;[F(a,b)=\sum^n&#95;&#123;k=1}(y_k-ax_k-b)^2&#92;]

Notice that the values &#92;(&#92;{\langle x_k,y_k\rangle:1\leq k\leq n&#92;}&#92;) are constants.
{:.info}

To solve this we use partial differentiation, finding &#92;(\left(\frac{\partial F}{\partial a},\frac{\partial F}{\partial b}\right)&#92;), and find the critical points for these.

### Best Line Fit Function
From that we find that the best line fit function is:

&#92;[\left(\frac{nW&#95;&#123;xy}-W_xW_y}{nW&#95;&#123;xx}-(W_x)^2}\right)x+\left(\frac{W&#95;&#123;xx}W_y-W&#95;&#123;xy}W_x}{nW&#95;&#123;xx}-(W_x)^2}\right)&#92;]

Where:

&#92;[
\begin{aligned}
W_x&=\sum^n&#95;&#123;k=1}x_k&#92;&#92;
W_y&=\sum^n&#95;&#123;k=1}y_k&#92;&#92;
W&#95;&#123;xx}&=\sum^n&#95;&#123;k=1}x_k^2&#92;&#92;
W_xy&=\sum^n&#95;&#123;k=1}x_ky_k&#92;&#92;
\end{aligned}
&#92;]

## Other Functions
Other functions can have the following forms:

* Powers - &#92;(ax^b&#92;)
* Exponentials - &#92;(ae^{bx}&#92;)
	* Also can be written as &#92;(a\exp(bx)&#92;).
* Logarithms - &#92;(a\ln x+b&#92;)
* Linear Rational Functions - &#92;(\frac1{ax+b}&#92;)

We may also have more than one parameter to optimise:

* Quadratic Functions - &#92;(ax^2+bx+c&#92;)

## Data Analysis Issues
The functions shown so far require manual selection and thus don't always exactly match the pairs of data that have been observed.

* **Lagrange interpolation** finds the best polynomial to fit a set of data.
* **Trigonometric interpolation** finds the best trigonometric curve to fit a set of data.
	* This is useful in signal processing.
