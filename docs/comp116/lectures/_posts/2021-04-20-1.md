---
title: Spectral Methods
tags: COMP116 Lectures
---
Spectral methods refer to the study of matrices.

## Identity Matrix
The identity matrix $I$ is a matrix which is all zero apart from the lead diagonal:

$$
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
\end{pmatrix}
$$

For any matrix $A$:

$$A\times I = I \times A = A$$

This is valid for square matrices.
{:.warning}

## Matrix Inverse
For an $n\times n$ matrix $A$ its inverse is the matrix denoted by $A^{-1}$ for which:

$$A\times A^{-1}=A^{-1}\times A=I$$

Not every matrix has an inverse.
{:.warning}

* **Singular** matrices have an inverse. 
* **Non-singular** matrices don't have an inverse. 

## Matrix Determinant

$$\det A \neq 0 \iff A^{-1} \text{ exists}$$

This is to say that $A$ is **non-singular**.
{:.info}

## Eigenvalues
Given an $n\times n$ matrix $A$, find all scalr values $\lambda$ for which ther is an $n$-vector $\vec x_\lambda$ satisfying:

$$A\times \vec x_\lambda = \lambda \vec x_\lambda$$

* These scalar values are called the **eigenvalues** of $A$.
* The $n$-vectors $\vec x_\lambda$ are called the **eigenvectors** of $A$ (with respect to $\lambda$).

The case $\vec x = \vec 0$ always produces a solution. Theretofore, in finding eigenvectors, this case is **not** considered.

The case $\lambda = 0$ **is** of interest.
{:.info}

In general, finding eigenvalues involves fiding $\lambda$ for which:

$$(A-\lambda I)\vec x_\lambda = \vec 0$$

This is the same as finding:

$$\det(A-\lambda I)=0$$

Notice that $(A-\lambda I)$ is the matrix $A$ with $\lambda$ subtracted from each diagonal entry.
{:.info}

### Solving via Polynomial Roots
The form $\det(A-\lambda I)=0$ has an interpretation involving roots of a polynomial.

Since $\det A$ (with some parameter $\lambda$) may be described as a polynomial of degree $n$ in $\lambda$ (with $A$ an $n\times n$ matrix) the solutions to $\det(A-\lambda I)=0$ are exactly the roots of this polynomial which is called the **characteristic polynomial** of $A$.

The characteristic polynomial of $A$ is denoted by: $\chi(A)$.
{:.info}

#### Consequences of Polynomial Similarity

1. An $n\times n$ matrix has $n$ eigenvalues (although these are not necessarily distinct).
	* This is similar to any other polynomial.
1. Eigenvalues may be complex numbers.
	* We won't be studying these.
	* Matrices with all real eigenvalues include **symmetric matrices**:
	
		$$[a_{ij}]=[a_{ji}]$$
		
### Ordering Eigenvalues and Dominance
Conventionally eigenvalues are written as:

$$\sigma(A)=(\lambda_1,\lambda_2,\ldots,\lambda_k,\ldots,\lambda_n)$$

This is called the **spectrum** of $A$.
{:.info}

* The eigenvalues are ordered from **largest** to **smallest**.
	* For complex values, the modulus is used.
* If $\lambda_1$ is **unique**, then it is called the **dominant eigenvalue**.
* The **largest** eigenvalue is referred to as the **spectral radius**.
* For **complex values** if $\lambda$ is an eigenvalue of $A$ then the conjugate, $\bar\lambda$, is too.