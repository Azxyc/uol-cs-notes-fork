---
title: COMP124 - Performance Considerations
tags: COMP124 Lectures
---
## Frame Allocation
There is a fixed amount of free memory that must be allocated among all processes.

* We needed to determine how many frames each process should get.

Each process will need a **minimum** number of pages.

* This is dependant on the architecture and its supported features.

Frames contain a number of pages.
{:.info}

### Allocation Schemes

* Equal Allocation:
	* Each process gets an **equal** share of frames.
* Proportional Allocation:
	* Allocate frames  according to the size of the process.
	* Could also implement **proportional allocation** based on process priorities.
	
		This avoids swapping key system processes.
		{:.info}

## Performance Considerations
Segmentation and paging overcomes may limitations of a linear store model but:

* There is a performance hit:
	* Each memory reference may require 2-3 memory access from table lookups.

Special hardware may help:

* Registers to hold the base address of the current code and data segments may allow tables to be bypassed.
* Special memory can aid fast table lookup:
	* Cache.

## Page Size
### Large Pages

* A **large** page size means a **smaller** page table.
* However large pages mean more wastage:
	* 50% of a page per process is lost due to internal fragmentation.

### Small Pages

* **Small** pages give better **resolution**.
	* Can bring in only the data that is needed fro the working set.
* Small pages increase the chances of page faults. 

## Memory Hierarchy

![](https://ace315dc-a-62cb3a1a-s-sites.googlegroups.com/site/cachememory2011/memory-hierarchy/hei.png)[1]

[1]: <https://sites.google.com/site/cachememory2011/memory-hierarchy> "Memory hierarchy diagram."

## Cache Memory
Efficiency can be maximised by keeping data as close to the CPU as possible:

* Registers are the best, but only few are available.
* Main memory is comparatively slow.
* Cache memory is small, fast memory that sits between the CPU and main memory.
	* It contains copies of items in main memory.
	
When the CPU needs information, it first checks the cache.

The Java `volatile` keyword prevents cache usage.
{:.info}

* If an item is found we have a cache **hit**.
* If not, we have a cache **miss**.
	* The block of data containing the requested item is copied into cache.
	
By the principle of locality, it is likely that future items will now be in cache.
{:.info}

* May have two or more levels of cache.
* May have separate caches for instructions and data.