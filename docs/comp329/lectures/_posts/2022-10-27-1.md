---
title: Landmark Detection
tags: COMP329 Lectures
---
We can also use the following models to localise a robot:

Map Matching (sonar, laser)
: Generate small, local maps from sensor data and match local maps against a global model.

Scan Matching (laser)
: Map is represented by scan endpoints, match scan into this map.

Beacons (sonar, laser, vision)
: Extract features such a doors hallways from sensor data.

## Landmarks & Beacons
This is the process of localising using beacons. They can be of the following types:

* Active beacons:
	* Radio tower
	* GPS
	* Code
* Passive:
	* Visual
	* Retro-reflective

This is different to previous sensor models as we are looking for specific landmarks:

* These observations can be used to estimate current location.
* The standard approach is to use **triangulation** or **trilateration**.

A sensor provides a combination of:

* Range $r$.
* Bearing $\phi$
* Signature $s$ which describes the feature:
	* Colour, height or type.
	
Landmarks should be of the following type:

* Be readily recognisable
* Perceivable from many different viewpoints.

### Landmarks
A feature vector represents an observed landmark. It is given as a collection of triples:

$$
f(z_t=\{f^1_t,f^2_t,\ldots\}=\left\{
\begin{pmatrix}r^1_t\\\phi^1_t,\\s^1_t\end{pmatrix},
\begin{pmatrix}r^2_t\\\phi^2_t,\\s^2_t\end{pmatrix},
\ldots\right\}
$$

By assuming conditional independence, we can consider one feature at a time such that:

$$
p(f(z_t)\mid x_t, m)=\prod p(r^i_t, \phi^i_t, s^i_t\mid x_t, m)
$$

We can combine this with map data in order to localise the robot.

A map is a list of objects and their locations:

$$
m=\{m_1, m_2, \ldots,m_N\}
$$

We are assuming that the map is **feature based** with landmarks.
{:.info}

Each feature may posses a signature and coordinate in global space $\langle m_{i,x},m_{i,y}$

We can model this with noise using a Gaussian on the range, bearing and signature.

#### Measuring the Range & Angle of a Beacon
We can:

* Include any details we can obtain on the signature.
* Assume that the range, angle and signature are independent

Given a robot pose $x_t=\langle x, y, \theta\rangle$:

$$
\begin{pmatrix}r^1_t\\\phi^1_t,\\s^1_t\end{pmatrix}=
\begin{pmatrix}
\sqrt{(m_{i,x}-x)^2+(m_{i,y}-y)^2}\\
\text{atan2}(m_{i,y}-y,m_{i,x})-\theta\\
s_i
\end{pmatrix}+
\begin{pmatrix}\epsilon_{\sigma^2_r}\\\epsilon_{\sigma^2_\phi}\\\epsilon_{\sigma^2_s}\end{pmatrix}
$$

This is implemented as an algorithm in [slides 104-105](https://canvas.liverpool.ac.uk/courses/59631/files/8251342/download?download_frd=1).
{:.info}

## Translateration
Given two or more landmarks, we can find the location and bearing of a robot.

There is an example of this in [slides 108-111](https://canvas.liverpool.ac.uk/courses/59631/files/8251342/download?download_frd=1).
{:.info}