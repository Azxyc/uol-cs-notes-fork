---
title: Image Compression
tags: ELEC319 Lectures
---
## Lossless Compression
Relies on probabilistic theory:

* Events that are less likely will contain more information.

A measure of average information is **entropy** also known as Shannon's Information Measure:

$$
H(x)=\sum_{i\in x}p(i)\log_2\left(\frac1{p(i)}\right)
$$

where:

* $x$ is a probabilistic experiment.
* $x_i$ are all the possible outcomes

The base of the logarithm is the unit of information (base 2 means that it is measured in bits).

### Entropy of an Image
The entropy $E$ of an $N\times N$ image is:

$$
E=-\sum^{L-1}_{i=0}p_i\log_2(p_i)
$$

where:

* $p_i$ is the probability of the $i^\text{th}$ greylevel.
* $L$ is the total number of greylevels.

additionally:

$$
0\leq E\leq \log_2L
$$

The average number of bits per pixel in a coder is:

$$
\hat L=\sum^{L-1}_{i=0}l_ip_i
$$

where:

* $p_i$ the probability of the $i^\text{th}$ greylevel.
* $l_i$ length in bits of the code for the $i^\text{th}$ greylevel.

### Image Entropy Example
Consider an image uses 3 bpp and the histogram is flat across the domain:

* As there are 3bbp there are 8 greylevels ($L=8$).
* Flat histogram means that the number of pixels at each greylevel are the same ($p_i\frac18$).

$$
\begin{aligned}
E&=-\sum^{L-1}_{i=0}p_i\log_2(p_i)\\
E&=-\sum^{7}_{i=0}\frac18\log_2(\frac18)\\
E&=3
\end{aligned}
$$

This means that there is no code that will provide better coding that what is used by default. An image with a flat histogram has the highest entropy.
{:.info}

## Huffman Coding
This is a type of coding that encodes frequent symbols as **short** strings of bits and infrequent symbols as longer bit strings.

* This is a minimum length code (it will generate close to the theoretical minimum).
* It generates variable length code.

Method:

1. **Sort** the symbols in decreasing order of probability.
1. **Combine** the smallest two by addition.
1. **Replace** the two elements with a single combined element.
1. Goto step 2 until only two probabilities are left.
1. **Work backwards** along the tree and generate the code by alternating 0 and 1.

### Huffman Coding Example
Consider a 10x10 image that uses 2bpp with the following histogram:

| Greylevel | Count |
| :-- | :-- |
| 0 | 20 |
| 1 | 30 |
| 2 | 10 | 
| 3 | 40 |

1. Find the probability of each level:

	| Greylevel | Probability |
	| :-- | :-- |
	| 0 | .2 |
	| 1 | .3 |
	| 2 | .1 | 
	| 3 | .4 |

1. Sort in descending order:

	| Greylevel | Probability |
	| :-- | :-- |
	| 3 | .4 |
	| 1 | .3 |
	| 0 | .2 |
	| 2 | .1 | 

1. Add the bottom two rows:

	| Original<br>Greylevel | Probability | Reduction 1 | Reduction 2 | Sort 2 |
	| :-- | :-- | :-- | :-- | :-- |
	| 3 | .4 | .4 | .4 | .6 |
	| 1 | .3 | .3 | .6 | .4 |
	| 0 | .2 | .3 | |
	| 2 | .1 | | |
	
	Sort between each iteration.
	{:.info}